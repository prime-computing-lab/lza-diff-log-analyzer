# LZA Diff Analyzer - LLM Configuration
# This file configures Large Language Model providers for enhanced analysis

default_provider: ollama
max_retries: 3
retry_delay: 1.0

# Fallback chain - providers will be tried in this order
fallback_chain:
  - ollama
  - openai
  - anthropic

providers:
  ollama:
    provider: ollama
    model: "your model"
    temperature: 0.1
    max_tokens: 4096
    timeout: 120
    base_url: "http://localhost:11434"
    enabled: true
    additional_params:
      num_predict: 4096
      top_k: 30
      top_p: 0.85
    
  # openai:
  #   provider: openai
  #   model: "gpt-4o-mini"
  #   temperature: 0.1
  #   max_tokens: 4096
  #   timeout: 30
  #   # api_key: ${OPENAI_API_KEY}  # Set via environment variable
  #   enabled: false  # Enable when API key is configured

  # anthropic:
  #   provider: anthropic
  #   model: "claude-3-haiku-20240307"
  #   temperature: 0.1
  #   max_tokens: 4096
  #   timeout: 30
  #   # api_key: ${ANTHROPIC_API_KEY}  # Set via environment variable
  #   enabled: false  # Enable when API key is configured

# Configuration notes:
# 1. To enable cloud providers, set the appropriate API keys as environment variables:
#    export OPENAI_API_KEY="your-openai-key"
#    export ANTHROPIC_API_KEY="your-anthropic-key"
#
# 2. For Ollama, ensure the service is running and the model is available:
#    ollama serve
#    ollama pull qwen2.5:7b
#
# 3. Recommended models for different use cases:
#    - Fast analysis: qwen2.5:7b, claude-3-haiku
#    - Detailed analysis: qwen2.5:14b, gpt-4o-mini
#    - Enterprise analysis: qwen2.5:32b, claude-3-sonnet
#
# 4. Adjust temperature (0.0-1.0) for creativity vs consistency:
#    - 0.1: Very consistent, deterministic
#    - 0.5: Balanced
#    - 0.9: More creative, varied responses

# RAG (Retrieval Augmented Generation) Configuration
rag:
  enabled: true
  vector_store:
    provider: chromadb
    collection_name: "lza_diff_logs"
    persist_directory: "./data/chromadb"
  embedding:
    model: "all-MiniLM-L6-v2"  # Lightweight, fast model
    cache_dir: "./data/embeddings_cache"
  retrieval:
    max_results: 10
    similarity_threshold: 0.6
    chunk_size: 2000
    chunk_overlap: 400
  performance:
    enable_caching: true
    cache_ttl: 3600  # 1 hour

# Conversation Management Configuration (AI Best Practices Aligned)
conversation:
  enabled: true
  max_history_messages: 20          # Admin configurable
  max_history_tokens: 4000          # Token-based limit
  retention_strategy: "sliding_window"  # sliding_window, semantic_compression, or fixed
  compression_threshold: 0.8        # When to compress old messages
  include_system_messages: false    # Whether to replay system prompts
  context_preservation:
    important_keywords: ["iam", "dependenciesstack", "security"]
    preserve_last_n_important: 5   # Always keep last N important exchanges
  resource_management:
    enable_token_counting: true     # Enable actual token counting
    enable_compression: false       # Enable message compression (requires more resources)
    lazy_loading: true              # Only load conversation history when needed
    memory_pooling: true            # Reuse message objects
  performance:
    async_processing: true          # Non-blocking conversation state management
    cache_token_counts: true        # Cache token counts to improve performance
    compression_cache_size: 100     # Number of compressed messages to cache
  debugging:
    log_retention_actions: true     # Log when messages are truncated/compressed
    conversation_metrics: true      # Track conversation statistics
    enable_conversation_export: false  # Enable conversation history export