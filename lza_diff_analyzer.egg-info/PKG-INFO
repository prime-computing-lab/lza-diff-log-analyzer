Metadata-Version: 2.4
Name: lza-diff-analyzer
Version: 0.1.0
Summary: LLM-powered AWS Landing Zone Accelerator diff log analyzer
Author: LZA Diff Analyzer Team
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: click>=8.1.0
Requires-Dist: rich>=13.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: jinja2>=3.1.0
Requires-Dist: pyyaml>=6.0.0
Requires-Dist: aiohttp>=3.8.0
Requires-Dist: requests>=2.31.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Provides-Extra: llm
Requires-Dist: openai>=1.0.0; extra == "llm"
Requires-Dist: anthropic>=0.25.0; extra == "llm"
Requires-Dist: ollama>=0.1.0; extra == "llm"
Provides-Extra: mcp
Requires-Dist: mcp>=0.1.0; extra == "mcp"

# LZA Diff Analyzer

An intelligent AWS Landing Zone Accelerator diff log analyzer powered by Large Language Models.

## Overview

This tool analyzes CloudFormation diff logs generated during AWS Landing Zone Accelerator (LZA) upgrades to identify potential risks, security implications, and provide actionable recommendations.

## Features

- **Cloud Administrator Friendly**: Human-readable summaries designed for cloud operations teams
- **Interactive Q&A Mode**: Ask natural language questions about your LZA changes
- **Multi-LLM Support**: Works with local models (via Ollama) with cloud API support planned (OpenAI, Anthropic)
- **Intelligent Risk Analysis**: Automated identification of high-risk changes with business context
- **LZA-Specific Intelligence**: Recognizes common LZA upgrade patterns and provides relevant guidance
- **Executive Summaries**: Clear explanations of what changes mean and what actions to take
- **Smart Recommendations**: Context-aware suggestions for change management and testing

## Installation

### Prerequisites

- Python 3.10+
- [UV](https://docs.astral.sh/uv/) (recommended for fast package management)

### Setup with UV (Recommended)

```bash
# Clone the repository
git clone <repository-url>
cd lza-diff-analyzer

# Install project with UV (creates venv automatically)
uv sync

# For all features (includes LLM and development tools)
uv sync --extra dev

# Activate the environment
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### Alternative Setup (Standard pip)

```bash
# Clone the repository
git clone <repository-url>
cd lza-diff-analyzer

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -e .

# For all features (includes LLM and development tools)  
pip install -e ".[dev]"
```

## Quick Start

```bash
# Ensure virtual environment is activated
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Basic analysis with cloud admin-friendly output (LLM enabled by default)
lza-analyze --input-dir /path/to/diff-logs

# Technical detailed output (for verbose analysis)
lza-analyze --input-dir /path/to/diff-logs --verbose

# Rule-based analysis only (disable LLM)
lza-analyze --input-dir /path/to/diff-logs --disable-llm

# Generate specific output format
lza-analyze --input-dir /path/to/diff-logs --format yaml

# Generate comprehensive HTML reports
lza-analyze --input-dir /path/to/diff-logs --generate-reports

# Specify a particular LLM provider
lza-analyze --input-dir /path/to/diff-logs --llm-provider anthropic
```

## Sample Output

### Cloud Administrator-Friendly Analysis
```
üîç EXECUTIVE SUMMARY
Your LZA upgrade from 1.10.0 to 1.12.1 affects 74 stacks across multiple accounts.
Total changes detected: 225 resources, 462 IAM modifications.

‚ö†Ô∏è  KEY CONCERNS (Requires Your Review):
1. IAM Permission Changes (258 findings)
   ‚îî‚îÄ Why: Roles and policies are being modified during upgrade
   ‚îî‚îÄ Action: Review cross-account access in DependenciesStack

üéØ IMMEDIATE ACTIONS:
‚ñ° Test IAM changes in non-production environment first
‚ñ° Review IAM changes for unintended privilege escalation
‚ñ° Validate automation scripts work with updated parameters
‚ñ° Backup current configurations before proceeding

üí° CONTEXT
This appears to be a standard LZA upgrade. High finding counts (331) are
normal due to IAM management complexity and version parameter updates.
Focus on the prioritized items above rather than the total count.
```

### Interactive Analysis Features
```bash
# The analyzer provides interactive analysis capabilities when LLM is enabled
$ lza-analyze --input-dir ./diff-logs --verbose

üîç EXECUTIVE SUMMARY
Your LZA upgrade from 1.10.0 to 1.12.1 affects 74 stacks across multiple accounts.
Total changes detected: 225 resources, 462 IAM modifications.

‚ö†Ô∏è KEY CONCERNS (Requires Your Review):
1. IAM Permission Changes (258 findings)
   ‚îî‚îÄ Why: Roles and policies are being modified during upgrade
   ‚îî‚îÄ Action: Review cross-account access in DependenciesStack

ü§ñ AI-POWERED INSIGHTS
The main IAM concerns in your LZA upgrade are:

1. **Cross-account role modifications** in DependenciesStack
   - These affect access between Log Archive, Audit, and workload accounts
   - Test in non-production to ensure connectivity remains intact

2. **Service role permission updates** 
   - LZA pipeline roles are getting new permissions for v1.12.1 features
   - Generally safe but verify no unexpected admin access

Recommended action: Compare role policies before/after in a test environment first.
```

## Testing

```bash
# Run tests (if available)
uv run pytest

# Run linting
uv run ruff check

# Run type checking
uv run mypy src/
```

## Development

This project is built in phases:

1. **Phase 1**: Foundation & Log Parsing ‚úÖ
2. **Phase 2**: LLM Abstraction Layer ‚úÖ (MCP Integration - Planned)
3. **Phase 3**: Risk Analysis Engine ‚úÖ (MCP Tools - Planned)
4. **Phase 4**: CLI & User Experience ‚úÖ (Interactive Mode - In Development)
5. **Phase 5**: Advanced Analysis & Reporting ‚úÖ (Advanced Features - In Development)

## Configuration

The LLM configuration is located at `config/llm_config.yaml`. Here's the current structure:

```yaml
# LZA Diff Analyzer - LLM Configuration
default_provider: ollama
max_retries: 3
retry_delay: 1.0

# Fallback chain - providers will be tried in this order
fallback_chain:
  - ollama
  # - openai     # Will be enabled when implementation is complete
  # - anthropic  # Will be enabled when implementation is complete

providers:
  ollama:
    provider: ollama
    model: "phi4:latest"  # ‚úÖ Fully supported - use your available model
    temperature: 0.1
    max_tokens: 4096
    timeout: 30
    base_url: "http://localhost:11434"
    enabled: true
    additional_params:
      num_predict: 4096
      top_k: 40
      top_p: 0.9

  # üîÑ Cloud providers - Implementation in progress:
  # openai:
  #   provider: openai
  #   model: "gpt-4o-mini"
  #   temperature: 0.1
  #   max_tokens: 4096
  #   timeout: 30
  #   api_key: ${OPENAI_API_KEY}  # Set via environment variable
  #   enabled: false

  # anthropic:
  #   provider: anthropic  
  #   model: "claude-3-haiku-20240307"
  #   temperature: 0.1
  #   max_tokens: 4096
  #   timeout: 30
  #   api_key: ${ANTHROPIC_API_KEY}  # Set via environment variable
  #   enabled: false
```

**Current LLM Support Status**:
- ‚úÖ **Ollama**: Fully implemented and production ready
- üîÑ **OpenAI**: Configuration ready, client implementation in progress  
- üîÑ **Anthropic**: Configuration ready, client implementation in progress

### Setting up Ollama

```bash
# Start Ollama service
ollama serve

# Check available models
ollama list

# Pull a model if needed (examples)
ollama pull qwen2.5:7b          # Fast, lightweight model
ollama pull qwen3:30b-a3b       # High-quality model (current default)
```

### Setting up Cloud Providers

```bash
# For OpenAI
export OPENAI_API_KEY="your-openai-api-key"

# For Anthropic
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```

## License

MIT License - see LICENSE file for details.
