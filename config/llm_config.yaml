# LZA Diff Analyzer - LLM Configuration
# This file configures Large Language Model providers for enhanced analysis

default_provider: ollama
max_retries: 3
retry_delay: 1.0

# Fallback chain - providers will be tried in this order
fallback_chain:
  - ollama
  - openai
  - anthropic

providers:
  ollama:
    provider: ollama
    model: "your model"
    temperature: 0.1
    max_tokens: 4096
    timeout: 120
    base_url: "http://localhost:11434"
    enabled: true
    additional_params:
      num_predict: 4096
      top_k: 30
      top_p: 0.85
    
  # openai:
  #   provider: openai
  #   model: "gpt-4o-mini"
  #   temperature: 0.1
  #   max_tokens: 4096
  #   timeout: 30
  #   # api_key: ${OPENAI_API_KEY}  # Set via environment variable
  #   enabled: false  # Enable when API key is configured

  # anthropic:
  #   provider: anthropic
  #   model: "claude-3-haiku-20240307"
  #   temperature: 0.1
  #   max_tokens: 4096
  #   timeout: 30
  #   # api_key: ${ANTHROPIC_API_KEY}  # Set via environment variable
  #   enabled: false  # Enable when API key is configured

# Configuration notes:
# 1. To enable cloud providers, set the appropriate API keys as environment variables:
#    export OPENAI_API_KEY="your-openai-key"
#    export ANTHROPIC_API_KEY="your-anthropic-key"
#
# 2. For Ollama, ensure the service is running and the model is available:
#    ollama serve
#    ollama pull qwen2.5:7b
#
# 3. Recommended models for different use cases:
#    - Fast analysis: qwen2.5:7b, claude-3-haiku
#    - Detailed analysis: qwen2.5:14b, gpt-4o-mini
#    - Enterprise analysis: qwen2.5:32b, claude-3-sonnet
#
# 4. Adjust temperature (0.0-1.0) for creativity vs consistency:
#    - 0.1: Very consistent, deterministic
#    - 0.5: Balanced
#    - 0.9: More creative, varied responses